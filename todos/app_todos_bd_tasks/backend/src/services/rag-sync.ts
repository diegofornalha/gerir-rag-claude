import { db } from '../db/client'
import { webfetchDocs } from '../db/schema/webfetch-docs'
import { eq, and, inArray } from 'drizzle-orm'
import { createHash } from 'crypto'
import fs from 'fs/promises'
import path from 'path'

interface RAGDocument {
  id: string
  content: string
  source: string
  metadata: {
    url?: string
    title?: string
    timestamp?: string
    type?: string
  }
}

interface CacheDocument {
  id: string
  source: string
  content: string
  embeddings?: number[]
  metadata: any
  timestamp: string
}

export class RAGSyncService {
  private cacheDir: string
  private documentsPath: string
  
  constructor() {
    this.cacheDir = path.join(process.env.HOME || '', '.claude', 'mcp-rag-cache')
    this.documentsPath = path.join(this.cacheDir, 'documents.json')
  }
  
  /**
   * Sincroniza cache local com PostgreSQL
   */
  async syncCacheToDatabase() {
    console.log('üîÑ Iniciando sincroniza√ß√£o RAG Cache ‚Üí PostgreSQL...')
    
    try {
      // 1. Ler documentos do cache local
      const cacheDocuments = await this.loadCacheDocuments()
      console.log(`üìö ${cacheDocuments.length} documentos encontrados no cache`)
      
      // 2. Separar por tipo
      const webDocs = cacheDocuments.filter(doc => 
        doc.metadata?.url || doc.source?.startsWith('http')
      )
      
      const sessionDocs = cacheDocuments.filter(doc => 
        doc.metadata?.type === 'session' || doc.source?.includes('.jsonl')
      )
      
      console.log(`üåê ${webDocs.length} documentos web`)
      console.log(`üìù ${sessionDocs.length} documentos de sess√£o`)
      
      // 3. Sincronizar documentos web
      await this.syncWebDocuments(webDocs)
      
      // 4. Por enquanto, apenas log das sess√µes (futura tabela)
      if (sessionDocs.length > 0) {
        console.log(`‚ÑπÔ∏è  Sess√µes ser√£o sincronizadas em futura implementa√ß√£o`)
      }
      
      console.log('‚úÖ Sincroniza√ß√£o conclu√≠da!')
      
      return {
        total: cacheDocuments.length,
        webDocs: webDocs.length,
        sessionDocs: sessionDocs.length
      }
      
    } catch (error) {
      console.error('‚ùå Erro na sincroniza√ß√£o:', error)
      throw error
    }
  }
  
  /**
   * Carrega documentos do cache local
   */
  private async loadCacheDocuments(): Promise<CacheDocument[]> {
    try {
      const data = await fs.readFile(this.documentsPath, 'utf-8')
      return JSON.parse(data)
    } catch (error) {
      console.warn('‚ö†Ô∏è  Cache n√£o encontrado ou vazio')
      return []
    }
  }
  
  /**
   * Sincroniza documentos web com a tabela webfetch_docs
   */
  private async syncWebDocuments(documents: CacheDocument[]) {
    for (const doc of documents) {
      try {
        const url = doc.metadata?.url || doc.source
        if (!url || !url.startsWith('http')) continue
        
        // Extrair informa√ß√µes
        const domain = new URL(url).hostname
        const contentHash = createHash('md5').update(doc.content).digest('hex')
        const wordCount = doc.content.split(/\s+/).length
        
        // Verificar se j√° existe
        const existing = await db
          .select()
          .from(webfetchDocs)
          .where(eq(webfetchDocs.url, url))
          .limit(1)
        
        if (existing.length > 0) {
          // Atualizar se mudou
          if (existing[0].contentHash !== contentHash) {
            await db
              .update(webfetchDocs)
              .set({
                contentHash,
                words: wordCount,
                documentId: doc.id,
                lastUpdated: new Date(),
                status: 'indexed',
                updatedAt: new Date()
              })
              .where(eq(webfetchDocs.id, existing[0].id))
            
            console.log(`üìù Atualizado: ${domain} - ${doc.metadata?.title || 'Sem t√≠tulo'}`)
          }
        } else {
          // Inserir novo
          await db.insert(webfetchDocs).values({
            url,
            domain,
            title: doc.metadata?.title || 'Documento RAG',
            description: doc.content.substring(0, 200) + '...',
            status: 'indexed',
            capturedAt: new Date(doc.timestamp || Date.now()),
            indexedAt: new Date(),
            contentHash,
            documentId: doc.id,
            words: wordCount,
            category: this.categorizeByDomain(domain),
            metadata: doc.metadata || {}
          })
          
          console.log(`‚úÖ Novo: ${domain} - ${doc.metadata?.title || 'Sem t√≠tulo'}`)
        }
        
      } catch (error) {
        console.error(`‚ùå Erro ao sincronizar ${doc.id}:`, error)
      }
    }
  }
  
  /**
   * Categoriza documento baseado no dom√≠nio
   */
  private categorizeByDomain(domain: string): string {
    const categories: Record<string, string[]> = {
      'MCP': ['modelcontextprotocol.io'],
      'Database': ['orm.drizzle.team', 'electric-sql.com', 'github.com/TanStack/db'],
      'Framework': ['fastify.dev', 'tanstack.com', 'zod.dev'],
      'Claude': ['anthropic.com', 'claude.ai'],
      'LocalFirst': ['localfirstweb.dev']
    }
    
    for (const [category, domains] of Object.entries(categories)) {
      if (domains.some(d => domain.includes(d))) {
        return category
      }
    }
    
    return 'General'
  }
  
  /**
   * Sincroniza do banco para o cache (dire√ß√£o inversa)
   */
  async syncDatabaseToCache() {
    console.log('üîÑ Sincronizando PostgreSQL ‚Üí RAG Cache...')
    
    try {
      // Buscar documentos indexados no banco
      const dbDocs = await db
        .select()
        .from(webfetchDocs)
        .where(eq(webfetchDocs.status, 'indexed'))
      
      console.log(`üìä ${dbDocs.length} documentos no banco`)
      
      // Carregar cache atual
      const cacheDocuments = await this.loadCacheDocuments()
      const cacheIds = new Set(cacheDocuments.map(d => d.id))
      
      // Identificar documentos que est√£o no banco mas n√£o no cache
      const missingInCache = dbDocs.filter(doc => 
        doc.documentId && !cacheIds.has(doc.documentId)
      )
      
      if (missingInCache.length > 0) {
        console.log(`‚ö†Ô∏è  ${missingInCache.length} documentos precisam ser re-indexados no cache`)
        // Aqui voc√™ poderia chamar o WebFetch para re-indexar
      }
      
      return {
        dbTotal: dbDocs.length,
        cacheTotal: cacheDocuments.length,
        missing: missingInCache.length
      }
      
    } catch (error) {
      console.error('‚ùå Erro na sincroniza√ß√£o reversa:', error)
      throw error
    }
  }
  
  /**
   * Status da sincroniza√ß√£o
   */
  async getSyncStatus() {
    const cacheDocuments = await this.loadCacheDocuments()
    const dbDocs = await db.select().from(webfetchDocs)
    
    return {
      cache: {
        total: cacheDocuments.length,
        types: this.groupByType(cacheDocuments)
      },
      database: {
        total: dbDocs.length,
        byStatus: this.groupByStatus(dbDocs),
        byCategory: this.groupByCategory(dbDocs)
      },
      lastSync: new Date()
    }
  }
  
  private groupByType(docs: CacheDocument[]) {
    const types: Record<string, number> = {}
    docs.forEach(doc => {
      const type = doc.metadata?.type || 'unknown'
      types[type] = (types[type] || 0) + 1
    })
    return types
  }
  
  private groupByStatus(docs: any[]) {
    const statuses: Record<string, number> = {}
    docs.forEach(doc => {
      statuses[doc.status] = (statuses[doc.status] || 0) + 1
    })
    return statuses
  }
  
  private groupByCategory(docs: any[]) {
    const categories: Record<string, number> = {}
    docs.forEach(doc => {
      const cat = doc.category || 'uncategorized'
      categories[cat] = (categories[cat] || 0) + 1
    })
    return categories
  }
}

// Singleton
export const ragSync = new RAGSyncService()